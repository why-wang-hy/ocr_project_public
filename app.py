# -*- coding: utf-8 -*-
"""
OCR & Translation App Backend
åŠŸèƒ½ï¼šå¤„ç†PDFä¸Šä¼ ã€Mistral OCRè¯†åˆ«ã€GitHubå›¾åºŠ/å­˜å‚¨ã€DeepSeekç¿»è¯‘
"""

import os
import time
import base64
import re
import traceback
import tempfile
import urllib.parse
import datetime
import requests
from flask import Flask, render_template, request, jsonify, url_for, Response
from mistralai import Mistral
from openai import OpenAI
from pypdf import PdfWriter, PdfReader
import threading # ğŸŸ¢ æ–°å¢ï¼šç”¨äºåå°å¼‚æ­¥æ‹‰å–
# ğŸŸ¢ å¿…é¡»æ·»åŠ è¿™ä¸€è¡Œï¼Œå¦åˆ™ä¼šæŠ¥â€œæœªå®šä¹‰ ThreadPoolExecutorâ€
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

app = Flask(__name__)

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„å˜é‡
load_dotenv()

# ä»ç¯å¢ƒå˜é‡ä¸­å®‰å…¨è¯»å– Key
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY", "")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "")

# ==============================================================================
# ğŸŸ¢ ç¬¬ä¸€éƒ¨åˆ†ï¼šå…¨å±€é…ç½®åŒºåŸŸ (Configuration)
# ==============================================================================

# --- GitHub é…ç½® (ç”¨äºäº‘ç«¯å­˜å‚¨å’Œå†å²è®°å½•) ---
GITHUB_USER = "why-wang-hy"
GITHUB_REPO = "ocr-team-docs"
GITHUB_BRANCH = "main"

# GitHub API æ„é€ 
GITHUB_API_BASE = f"https://api.github.com/repos/{GITHUB_USER}/{GITHUB_REPO}/contents"
GH_HEADERS = {
    "Authorization": f"token {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

# --- App è¿è¡Œå‚æ•° ---
PAGE_CHUNK_SIZE = 5  # PDF å¤„ç†åˆ†å—å¤§å°ï¼ˆæ¯5é¡µä¸€ç»„ï¼‰
BASE_STATIC_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'static')

# --- ç”¨æˆ·èº«ä»½æ˜ å°„ ---
USERS = {
    's1': 'ç‹æµ©æ‡¿',
    's2': 'ç‹ç‰§è™¹',
    's3': 'é™ˆå¦¤ä½•',
    's4': 'åŒä¼´ D',
    's5': 'åŒä¼´ E',
    's6': 'åŒä¼´ F',
    's7': 'åŒä¼´ G',
    's8': 'é»˜è®¤'
}

#ğŸŸ¢ æ–°å¢ï¼šå…¨å±€ç¼“å­˜å®¹å™¨
# ç»“æ„: { 's1': [...åˆ—è¡¨æ•°æ®...], 's2': [...] }
HISTORY_CACHE = {}

# ==============================================================================
# ğŸŸ¢ ç¬¬äºŒéƒ¨åˆ†ï¼šGitHub å·¥å…·æ¨¡å— (GitHub Utils)
# ==============================================================================

def upload_to_github(file_path, target_path, commit_message):
    """
    åŠŸèƒ½ï¼šå°†æœ¬åœ°æ–‡ä»¶ä¸Šä¼ åˆ° GitHub æŒ‡å®šä»“åº“è·¯å¾„ã€‚
    
    :param file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
    :param target_path: GitHub ä»“åº“å†…çš„ç›®æ ‡è·¯å¾„
    :param commit_message: æäº¤ä¿¡æ¯
    :return: Boolean (æˆåŠŸä¸º True)
    """
    try:
        # 1. è¯»å–æ–‡ä»¶å¹¶è½¬æ¢ä¸º Base64
        with open(file_path, "rb") as f:
            content = base64.b64encode(f.read()).decode("utf-8")
        
        # 2. æ„é€  API URL (å¤„ç†è·¯å¾„ä¸­çš„ç‰¹æ®Šå­—ç¬¦)
        url = f"{GITHUB_API_BASE}/{urllib.parse.quote(target_path)}"
        
        # 3. æ„é€ è¯·æ±‚ä½“
        data = {
            "message": commit_message,
            "content": content,
            "branch": GITHUB_BRANCH
        }
        
        # 4. å‘é€ PUT è¯·æ±‚
        resp = requests.put(url, json=data, headers=GH_HEADERS)
        
        if resp.status_code in [200, 201]:
            return True
        else:
            print(f"GitHub Upload Failed: {resp.text}")
            return False
    except Exception as e:
        print(f"Upload Error: {e}")
        return False
    
# ==================== ğŸŸ¢ æå–ï¼šç‹¬ç«‹çš„ GitHub è·å–å‡½æ•° ====================
# è¿™ä¸ªå‡½æ•°è´Ÿè´£å¹²è„æ´»ç´¯æ´»ï¼Œä¸ç›´æ¥å¤„ç† HTTP è¯·æ±‚ï¼Œæ–¹ä¾¿è¢«å„ç§è·¯ç”±è°ƒç”¨
def _fetch_github_data(user_id):
    """
    åŠŸèƒ½ï¼šè¿æ¥ GitHub API è·å–åŸå§‹æ•°æ®ï¼Œè®¡ç®—æ—¶é—´æˆ³ï¼Œè¿”å›å¤„ç†åçš„åˆ—è¡¨ã€‚
    æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªè€—æ—¶æ“ä½œ (1-3ç§’)ã€‚
    """
    contents_url = f"{GITHUB_API_BASE}/{user_id}"
    print(f"ğŸ”„ [Cache Worker] æ­£åœ¨åå°æ‹‰å– {user_id} çš„æ•°æ®...")
    
    try:
        # 1. è·å–æ–‡ä»¶åˆ—è¡¨
        resp = requests.get(contents_url, headers=GH_HEADERS)
        if resp.status_code != 200: 
            print(f"âš ï¸ [Cache Worker] è·å–åˆ—è¡¨å¤±è´¥: {resp.status_code}")
            return []

        items = resp.json()
        if not isinstance(items, list): return []

        # 1. ç¬¬ä¸€æ­¥ï¼šå…ˆæ‰«ææ‰€æœ‰æ–‡ä»¶ï¼ŒæŒ‰åŸå§‹ PDF åç§°å½’ç±»
        # ç»“æ„ï¼š{ "æ–‡ä»¶å": { "pdf": path, "mds": [{"name": "æ˜¾ç¤ºå", "path": path}], "time": 0 } }
        files_groups = {}

        for item in items:
            if item['type'] != 'file': continue
            full_name = item['name']
            path = item['path']
            base_name, ext = os.path.splitext(full_name)
            ext = ext.lower()

            # åˆ¤æ–­æ˜¯å¦æ˜¯åŒè¯­ç‰ˆ
            is_dual = base_name.endswith('_dual')
            # ç»Ÿä¸€æ‰¾å›åŸå§‹ PDF çš„ base_name (å»æ‰ _dual)
            origin_base = base_name.replace('_dual', '') if is_dual else base_name

            if origin_base not in files_groups:
                files_groups[origin_base] = {'pdf': None, 'mds': [], 'timestamp': 0}

            if ext in ['.pdf', '.jpg', '.png']:
                files_groups[origin_base]['pdf'] = path
            elif ext == '.md':
                display_name = f"{origin_base} (åŒè¯­)" if is_dual else origin_base
                files_groups[origin_base]['mds'].append({
                    'display_name': display_name,
                    'path': path
                })

        # --- ğŸŸ¢ æ ¸å¿ƒä¿®æ”¹éƒ¨åˆ†ï¼šä»…è¯·æ±‚å‰ 7 ä¸ªè®°å½•çš„æ—¶é—´æˆ³ ---
        # 1. è·å–æ‰€æœ‰æœ‰ PDF çš„ç»„å
        group_keys = [k for k, v in files_groups.items() if v['pdf']]
        
        # 2. è¿™é‡Œçš„ group_keys é¡ºåºé€šå¸¸æ˜¯ GitHub è¿”å›çš„é¡ºåºï¼ˆé€šå¸¸æŒ‰åç§°æ’åºï¼‰
        # æˆ‘ä»¬å–å‰ 7 ä¸ªè¿›è¡Œæ—¶é—´æˆ³è¯·æ±‚
        for i, origin_base in enumerate(group_keys):
            if i >= 7: break # è¶…è¿‡ 7 ä¸ªåˆ™è·³è¿‡è¯·æ±‚ï¼Œä¿æŒ timestamp ä¸º 0
            
            data = files_groups[origin_base]
            try:
                commit_url = f"https://api.github.com/repos/{GITHUB_USER}/{GITHUB_REPO}/commits"
                c_resp = requests.get(commit_url, 
                                    params={'path': data['pdf'], 'per_page': 1},
                                    headers=GH_HEADERS)
                if c_resp.status_code == 200 and c_resp.json():
                    date_str = c_resp.json()[0]['commit']['committer']['date']
                    data['timestamp'] = datetime.datetime.fromisoformat(date_str.replace('Z', '+00:00')).timestamp()
            except Exception as e:
                print(f"âš ï¸ è·å–æ—¶é—´æˆ³å¤±è´¥ ({origin_base}): {e}")

        # 3. å±•å¹³å¹¶æ„å»ºæœ€ç»ˆåˆ—è¡¨
        history_items = []
        for origin_base, data in files_groups.items():
            if data['pdf'] and data['mds']:
                for md_info in data['mds']:
                    history_items.append({
                        'name': md_info['display_name'],
                        'pdf_path': data['pdf'],
                        'md_path': md_info['path'],
                        'timestamp': data['timestamp']
                    })
        
        # æŒ‰æ—¶é—´æˆ³é™åºæ’åºï¼ˆæœ€è¿‘çš„åœ¨å‰ï¼‰
        history_items.sort(key=lambda x: x['timestamp'], reverse=True)
        
        # æ›´æ–°ç¼“å­˜
        history_manager.set(user_id, history_items)
        print(f"âœ… [Cache Worker] {user_id} ç¼“å­˜å·²æ›´æ–°ï¼Œè¯·æ±‚äº†å‰ 7 é¡¹æ—¶é—´æˆ³")
        return history_items

    except Exception as e:
        print(f"âŒ [Cache Worker] Error: {e}")
        return []
    
# ==================== ğŸŸ¢ æ–°å¢ï¼šåå°åˆ·æ–°ä»»åŠ¡ ====================
def background_refresh_task(user_id):
    """çº¿ç¨‹å…¥å£å‡½æ•°"""
    with app.app_context(): # ç¡®ä¿æœ‰ Flask ä¸Šä¸‹æ–‡ï¼ˆè™½ç„¶è¿™é‡Œä¸»è¦ç”¨ requestsï¼‰
        _fetch_github_data(user_id)

# ğŸŸ¢ ä¿®æ”¹ç¬¬ä¸€éƒ¨åˆ†ï¼šæ”¹è¿›ç¼“å­˜é€»è¾‘
# ä½¿ç”¨ä¸€ä¸ªå¸¦é”çš„ç±»æ¥ç®¡ç†ç¼“å­˜ï¼Œé˜²æ­¢å¤šçº¿ç¨‹ç«äº‰ï¼Œå¹¶å¢åŠ ç®€å•çš„æœ¬åœ°æŒä¹…åŒ–ï¼ˆå¯é€‰ï¼‰
class HistoryManager:
    def __init__(self):
        self.cache = {}
        self.last_sync = {}
        self.lock = threading.Lock()

    def get(self, user_id):
        with self.lock:
            return self.cache.get(user_id)

    def set(self, user_id, data):
        with self.lock:
            self.cache[user_id] = data
            self.last_sync[user_id] = time.time()

history_manager = HistoryManager()

# ==============================================================================
# ğŸŸ¢ ç¬¬ä¸‰éƒ¨åˆ†ï¼šç¿»è¯‘å¼•æ“æ¨¡å— (Translation Engine - Advanced Isolation)
# ==============================================================================

class ContentIsolator:
    """
    åŠŸèƒ½ï¼šä¸“é—¨è´Ÿè´£å†…å®¹çš„ æå–(Protect) ä¸ è¿˜åŸ(Restore)
    ç­–ç•¥ï¼šç»´æŠ¤ä¸€ä¸ªæœ‰åºçš„æ›¿æ¢åˆ—è¡¨ï¼Œç¡®ä¿åµŒå¥—ç»“æ„è¢«æ­£ç¡®å¤„ç†
    """
    def __init__(self):
        self.vault = {} # å­˜å‚¨åŸå§‹å†…å®¹: {'key': 'content'}
        self.counter = 0
    
    def _get_key(self, prefix):
        """ç”Ÿæˆå”¯ä¸€çš„å ä½ç¬¦ Key"""
        key = f"[[__{prefix}_{self.counter}__]]"
        self.counter += 1
        return key

    def protect(self, text, pattern, prefix):
        """
        é€šç”¨ä¿æŠ¤å‡½æ•°
        :param text: æ–‡æœ¬
        :param pattern: æ­£åˆ™è¡¨è¾¾å¼
        :param prefix: å ä½ç¬¦å‰ç¼€ (å¦‚ IMG, EQ, TBL)
        """
        def replacer(match):
            content = match.group(0)
            key = self._get_key(prefix)
            self.vault[key] = content
            return key
        
        return re.sub(pattern, replacer, text, flags=re.MULTILINE | re.DOTALL)

    def restore(self, text):
        """å°†å ä½ç¬¦è¿˜åŸä¸ºåŸå§‹å†…å®¹"""
        # ä¸ºäº†é˜²æ­¢å¶å‘çš„åµŒå¥—æ›¿æ¢é—®é¢˜ï¼Œå»ºè®®æŒ‰ Key çš„é•¿åº¦é€†åºè¿˜åŸï¼Œæˆ–è€…ç›´æ¥éå†
        # è¿™é‡Œç”±äº Key æ ¼å¼å›ºå®šï¼Œç›´æ¥éå†å³å¯
        for key, content in self.vault.items():
            # ä½¿ç”¨ replace è€Œé re.subï¼Œé˜²æ­¢ content ä¸­åŒ…å«æ­£åˆ™æ•æ„Ÿå­—ç¬¦å¯¼è‡´å´©æºƒ
            text = text.replace(key, content)
        return text

class SafeTranslator:
    """
    åŠŸèƒ½ï¼šå­¦æœ¯ç¿»è¯‘å¼•æ“ (Pro ç‰ˆ)
    ç‰¹ç‚¹ï¼šå½»åº•éš”ç¦»å›¾ç‰‡ã€ä»£ç ã€å…¬å¼ã€è¡¨æ ¼ï¼Œåªç¿»è¯‘çº¯æ–‡æœ¬
    """
    def __init__(self):
        self.client = OpenAI(
            api_key=DEEPSEEK_API_KEY, 
            base_url="https://api.deepseek.com"
        )

    def translate_bilingual(self, markdown_text):
        if not markdown_text.strip():
            return ""

        # å®ä¾‹åŒ–éš”ç¦»å™¨ (æ¯ä¸ª Chunk ç‹¬ç«‹)
        isolator = ContentIsolator()
        processed_text = markdown_text

        # ========== ğŸ›¡ï¸ éš”ç¦»é˜¶æ®µ (é¡ºåºå¾ˆé‡è¦!) ==========
        
        # 1. ä¿æŠ¤ä»£ç å— (```...```) - ä¼˜å…ˆçº§æœ€é«˜
        # è¯´æ˜ï¼šé˜²æ­¢ä»£ç é‡Œçš„æ•°å­¦ç¬¦å·æˆ–å›¾ç‰‡æ ‡è®°è¢«è¯¯è¯†åˆ«
        processed_text = isolator.protect(
            processed_text, 
            r'```[\s\S]*?```', 
            "CODE"
        )

        # 2. ä¿æŠ¤å›¾ç‰‡ (![...](...)) 
        # è¯´æ˜ï¼šé˜²æ­¢ Base64 å¹²æ‰°ç¿»è¯‘ï¼ŒåŒæ—¶é˜²æ­¢æ¨¡å‹ä¿®æ”¹å›¾ç‰‡è·¯å¾„
        processed_text = isolator.protect(
            processed_text, 
            r'!\[.*?\]\(.*?\)', 
            "IMG"
        )

        # 3. ä¿æŠ¤ Markdown è¡¨æ ¼
        # ç‰¹å¾ï¼šåŒ¹é…è¿ç»­çš„ä»¥ | å¼€å¤´çš„è¡Œã€‚è¿™èƒ½é˜²æ­¢è¡¨æ ¼ç»“æ„è¢«ç¿»è¯‘æ‰“ä¹±ã€‚
        # æ³¨æ„ï¼šè¿™æ„å‘³ç€è¡¨æ ¼å†…çš„æ–‡å­—å°†ä¸ä¼šè¢«ç¿»è¯‘ï¼ˆé€šå¸¸OCRçš„è¡¨æ ¼ç¿»è¯‘åæ ¼å¼æéš¾æ§åˆ¶ï¼Œå»ºè®®ä¿ç•™åŸæ–‡ï¼‰
        processed_text = isolator.protect(
            processed_text,
            r'(?:^\|.*?\|\s*$\n?)+',
            "TBL"
        )

        # 4. ä¿æŠ¤ Block å…¬å¼ ($$ ... $$)
        processed_text = isolator.protect(
            processed_text,
            r'\$\$[\s\S]*?\$\$',
            "EQ_BLOCK"
        )

        # 5. ä¿æŠ¤ Inline å…¬å¼ ($ ... $)
        # è¯´æ˜ï¼šä½¿ç”¨è´Ÿå‘é¢„æŸ¥ (?<!\\) é˜²æ­¢åŒ¹é…è½¬ä¹‰çš„ \$
        processed_text = isolator.protect(
            processed_text,
            r'(?<!\\)\$(?!\s).*?(?<!\s)(?<!\\)\$',
            "EQ_INLINE"
        )

        # æ„é€  System Prompt (é’ˆå¯¹æ–°å ä½ç¬¦ä¼˜åŒ–)
        system_prompt = r"""
            ä½ æ˜¯ä¸€ä½ç²¾é€šæ•°å­¦å»ºæ¨¡ä¸ç§‘å­¦ç ”ç©¶çš„å­¦æœ¯ç¿»è¯‘ä¸“å®¶ã€‚ä½ è´Ÿè´£å°†å¤æ‚çš„å­¦æœ¯ Markdown æ–‡æ¡£ä»è‹±æ–‡ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œå¹¶ä¿æŒæ–‡æ¡£çš„ä¸¥è°¨æ€§ä¸æ’ç‰ˆå®Œæ•´æ€§ã€‚

            ### ğŸ“ ç¿»è¯‘è§„èŒƒä¸æ ¼å¼è¦æ±‚ (å¿…é¡»éµå®ˆ)ï¼š
            1. **åŒè¯­å¯¹ç…§æ ¼å¼**ï¼šé‡‡ç”¨â€œé€æ®µå¯¹ç…§â€åŸåˆ™ã€‚è¾“å‡ºæ¯ä¸€æ®µåŸæ–‡åï¼Œç´§è·Ÿå…¶å¯¹åº”çš„ä¸­æ–‡ç¿»è¯‘æ®µè½ã€‚
            2. **è¯‘æ–‡å¼•ç”¨æ ‡è¯†**ï¼šæ‰€æœ‰çš„ä¸­æ–‡ç¿»è¯‘æ®µè½å¿…é¡»ä¸”åªèƒ½åŒ…è£¹åœ¨ Markdown å¼•ç”¨å—å†…ï¼Œå³ä»¥ `> ` å¼€å¤´ã€‚
            3. **æœ¯è¯­å‡†ç¡®æ€§**ï¼šä½¿ç”¨åœ°é“çš„ä¸­å›½å­¦æœ¯è¯­ç”¨ä¹ æƒ¯ï¼ˆå¦‚â€œæœ¬æ–‡â€ã€â€œæ˜¾è‘—æ€§â€ã€â€œé²æ£’æ€§â€ç­‰ï¼‰ã€‚
            4. **å ä½ç¬¦ä¿ç•™**ï¼š
               - æ–‡æœ¬ä¸­åŒ…å«ç±»ä¼¼ `[[__IMG_n__]]` (å›¾ç‰‡)ã€`[[__TBL_n__]]` (è¡¨æ ¼)ã€`[[__EQ_BLOCK_n__]]` (å—çº§å…¬å¼)ã€`[[__EQ_INLINE_n__]]` (è¡Œå†…å…¬å¼) ä»¥åŠ `[[__PB_n__]]` (æ¢é¡µç¬¦) çš„å ä½ç¬¦ã€‚
               - è¿™äº›å ä½ç¬¦åœ¨è¯‘æ–‡ä¸­å¿…é¡»**åŸæ ·ä¿ç•™**ï¼Œä½ç½®åº”ç¬¦åˆä¸­æ–‡è¯­åºã€‚

            ### ğŸš« ç»å¯¹ç¦ä»¤ (è¿è€…å°†å¯¼è‡´è§£æå´©æºƒ)ï¼š
            1. **ä¸¥ç¦ä¿®æ”¹å ä½ç¬¦ç»“æ„**ï¼š
               - ä¸¥ç¦ç¿»è¯‘å ä½ç¬¦å†…éƒ¨çš„è‹±æ–‡ï¼ˆå¦‚æŠŠ IMG ç¿»è¯‘æˆâ€œå›¾ç‰‡â€ï¼‰ã€‚
               - ä¸¥ç¦åœ¨å ä½ç¬¦çš„å¤§æ‹¬å·å†…éƒ¨æ·»åŠ ä»»ä½•ç©ºæ ¼ã€‚
               - âœ… æ­£ç¡®ï¼š`> è¯¥æ¨¡å‹å¦‚ [[__IMG_0__]] æ‰€ç¤ºã€‚`
               - âŒ é”™è¯¯ï¼š`> è¯¥æ¨¡å‹å¦‚ [[ __å›¾ç‰‡_0__ ]] æ‰€ç¤ºã€‚`

            2. **ä¸¥ç¦åœ¨è¯‘æ–‡ä¸­ä½¿ç”¨å…¬å¼å®šç•Œç¬¦**ï¼š
               - ä¸¥ç¦åœ¨ `> ` å¼€å¤´çš„è¯‘æ–‡ä¸­è¾“å‡º `$$`ã€`\[`ã€`\]`ã€`\begin{...}` æˆ– `\end{...}`ã€‚æ‰€æœ‰å…¬å¼å¿…é¡»é€šè¿‡å¯¹åº”çš„ `[[__EQ_...__]]` å ä½ç¬¦ä½“ç°ã€‚

            3. **ç¦æ­¢ç¿»è¯‘çº¯ç»„ä»¶è¡Œ**ï¼š
               - å¦‚æœåŸæ–‡æ®µè½åªåŒ…å«å ä½ç¬¦ï¼ˆå¦‚åªæœ‰ `[[__EQ_BLOCK_0__]]`ï¼‰è€Œæ— æ–‡å­—å†…å®¹ï¼Œ**ä¸¥ç¦**è¾“å‡ºå¯¹åº”çš„ `> ` è¯‘æ–‡è¡Œï¼Œç›´æ¥è·³è¿‡å¹¶å¤„ç†ä¸‹ä¸€æ®µã€‚

            4. **ç¦æ­¢ç¿»è¯‘å­¤ç«‹å™ªå£°**ï¼š
               - é‡åˆ°å•ç‹¬çš„é¡µç æ•°å­—ï¼ˆå¦‚ '1'ï¼‰ã€å¹´ä»½ï¼ˆå¦‚ '2025'ï¼‰æˆ– OCR äº§ç”Ÿçš„ä¼ªå½±æ•°å­—ï¼Œè¯·ç›´æ¥å¿½ç•¥ï¼Œä¸è¦è¾“å‡ºç¿»è¯‘ã€‚

            5. **ä¿æŠ¤ Markdown è¯­æ³•å…ƒå­—ç¬¦**ï¼š
               - ä¸¥ç¦ä¿®æ”¹åŸæ–‡ä¸­çš„æ ‡é¢˜çº§æ•°ï¼ˆ`#`ï¼‰ã€åˆ—è¡¨ç¬¦å·ï¼ˆ`-`ã€`1.`ï¼‰æˆ–åŠ ç²—ç¬¦å·ï¼ˆ`**`ï¼‰ã€‚

            ### ğŸ’¡ ç¤ºä¾‹å±•ç¤ºï¼š
            è¾“å…¥ï¼š
            # 1. Introduction
            The growth of fungi is modeled by [[__EQ_INLINE_0__]].
            [[__EQ_BLOCK_1__]]

            è¾“å‡ºï¼š
            # 1. Introduction
            > # 1. ç»ªè®º

            The growth of fungi is modeled by [[__EQ_INLINE_0__]].
            > çœŸèŒçš„ç”Ÿé•¿é€šè¿‡ [[__EQ_INLINE_0__]] è¿›è¡Œå»ºæ¨¡ã€‚

            [[__EQ_BLOCK_1__]]
            (æ­¤å¤„ä¸è¾“å‡ºè¯‘æ–‡ï¼Œå› ä¸ºè¯¥æ®µä»…åŒ…å«å—çº§å…¬å¼å ä½ç¬¦)
            """

        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": processed_text}
                ],
                stream=False,
                temperature=0.1 # é™ä½éšæœºæ€§ï¼Œç¡®ä¿å ä½ç¬¦ä¸ä¹±è·‘
            )
            translated_text = response.choices[0].message.content
        except Exception as e:
            print(f"âŒ API Error: {e}")
            return f"{markdown_text}\n\n> âš ï¸ ç¿»è¯‘æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {e}"

        # ========== ğŸ”„ è¿˜åŸé˜¶æ®µ (æ”¹è¿›ç‰ˆ) ==========
        
        # 1. åˆ†ç¦»åŸæ–‡å’Œè¯‘æ–‡å— (å‡è®¾ AI éµå¾ªäº† > å¼•ç”¨å—æ ¼å¼)
        # æˆ‘ä»¬éœ€è¦åˆ†åˆ«å¤„ç†ï¼šåŸæ–‡ä¿ç•™æ‰€æœ‰å ä½ç¬¦ï¼Œè¯‘æ–‡åˆ æ‰éæ–‡å­—å ä½ç¬¦
        lines = translated_text.split('\n')
        final_lines = []
        
        for line in lines:
            if line.strip().startswith('>'):
                # è¿™æ˜¯è¯‘æ–‡è¡Œï¼šæˆ‘ä»¬è¦åœ¨è¿™é‡Œåˆ æ‰ç»„ä»¶å ä½ç¬¦
                # å‰”é™¤å›¾ç‰‡ã€è¡¨æ ¼ã€å—çº§å…¬å¼å ä½ç¬¦ï¼Œåªä¿ç•™æ–‡å­—
                clean_line = line
                # åˆ æ‰å›¾ç‰‡
                clean_line = re.sub(r'\[\[__IMG_\d+__\]\]', '', clean_line)
                # åˆ æ‰è¡¨æ ¼
                clean_line = re.sub(r'\[\[__TBL_\d+__\]\]', '', clean_line)
                # åˆ æ‰å—çº§å…¬å¼ (å¯é€‰ï¼šå¦‚æœä½ å¸Œæœ›è¯‘æ–‡é‡Œä¹Ÿä¸è¦è¡Œå†…å…¬å¼ï¼Œå¯ä»¥ä¸€å¹¶åˆ æ‰)
                clean_line = re.sub(r'\[\[__EQ_BLOCK_\d+__\]\]', '', clean_line)
                
                # è¿˜åŸå‰©ä¸‹çš„æ–‡å­—å ä½ç¬¦ (å¦‚æœæœ‰çš„è¯)
                final_lines.append(isolator.restore(clean_line))
            else:
                # è¿™æ˜¯åŸæ–‡è¡Œï¼šå®Œå…¨è¿˜åŸï¼Œä¿ç•™æ‰€æœ‰ç»„ä»¶
                final_lines.append(isolator.restore(line))
        
        return '\n'.join(final_lines)
        
def is_likely_toc(text):
    """æ£€æµ‹æ˜¯å¦ä¸ºç›®å½•é¡µï¼šå¯»æ‰¾â€˜æ ‡é¢˜...æ•°å­—â€™ç‰¹å¾"""
    toc_lines = re.findall(r'^[^\n]{5,}\s+\d+$', text, re.MULTILINE)
    return len(toc_lines) >= 3 #
        
# ==================== ğŸŸ¢ æ ¸å¿ƒä¿®å¤ï¼šåç«¯æ–‡æœ¬æ¸…æ´— ====================
def backend_smart_clean(content):
    if not content: return ""

    # 1. ğŸŸ¢ å›¾ç‰‡â€œä¿é™©ç®±â€éš”ç¦»ï¼šé˜²æ­¢å·¨å¤§çš„ Base64 å­—ç¬¦ä¸²è¢«ä¸‹æ–¹çš„æ­£åˆ™è¯¯åˆ æˆ–å¯¼è‡´å¡é¡¿
    imgs = []
    def _hide(m):
        imgs.append(m.group(0))
        return f"__IMG_TMP_{len(imgs)-1}__"
    
    # åŒ¹é…æ‰€æœ‰çš„ Markdown å›¾ç‰‡æ ‡ç­¾ (å« Base64)
    content = re.sub(r'!\[.*?\]\(data:image\/.*?;base64,.*?\)', _hide, content)
    
    # 2. ğŸŸ¢ ç»ˆæå…¬å¼ä¿®å¤ï¼šæš´åŠ›è¿˜åŸ HTML å®ä½“
    # è¿™é‡Œä½¿ç”¨é¡ºåºæ›¿æ¢ï¼Œå…ˆå¤„ç†äºŒæ¬¡è½¬ä¹‰ï¼Œå†å¤„ç†æ ‡å‡†è½¬ä¹‰
    content = content.replace('&amp;lt;', '<').replace('&lt;', '<')
    content = content.replace('&amp;gt;', '>').replace('&gt;', '>')
    content = content.replace('&amp;le;', r'\le').replace('&le;', r'\le')
    content = content.replace('&amp;ge;', r'\ge').replace('&ge;', r'\ge')
    content = content.replace('&amp;plusmn;', r'\pm').replace('&plusmn;', r'\pm')

    # 3. ğŸŸ¢ ä¿®å¤çŸ©é˜µè¯­æ³• (ç§»é™¤ \begin{array}[] è¿™ç§éæ ‡æ ‡è®°)
    # ä½¿ç”¨ re.DOTALL ç¡®ä¿èƒ½è·¨è¿‡æ¢è¡Œç¬¦åŒ¹é…æ–¹æ‹¬å·
    content = re.sub(r'\\begin\{array\}\s*\[.*?\]', r'\\begin{array}', content, flags=re.DOTALL)
    content = content.replace('[]{cccccc}', '{cccccc}')

    # 4. ğŸŸ¢ ç§»é™¤ OCR åƒåœ¾ä¿¡æ¯ (åŒæ­¥å‰ç«¯é€»è¾‘)
    ad_keywords = [
        'è·å–æ›´å¤šèµ„è®¯', 'ä¼˜è´¨æ›´å¤šèµ„è®¯', 'åœ‹ç«‹è‡ºç£å¤§å­¸','æ•°å­—æ¨¡å‹', 'æ•°å­¦æ¨¡å‹','I would like to get more information.', 
        'ä¸Šæµ·', 'å¤©æ´¥', 'æ–‡æ±‡', 'äº‘æ±Ÿ', 'å¤ªæ±Ÿ', 'äº‘è®¡', 'äº¤å¾€','æ–‡æ±Ÿ','è³‡è¨Š','å¤§æ±Ÿ','å…³æ³¨æ•°å­¦'
    ]
    ad_regex = r'^.*(' + '|'.join(ad_keywords) + r').*$'
    content = re.sub(ad_regex, '', content, flags=re.MULTILINE)
    
    # ç§»é™¤ Team æ ‡è®°ä¸ Page é¡µç 
    content = re.sub(r'^Team\s*[#]?\s*\d+\s*.*$', '', content, flags=re.MULTILINE)
    content = re.sub(r'^Page\s+\d+(?:\s+of\s+\d+)?\s*.*$', '', content, flags=re.MULTILINE)
    content = re.sub(r'[â†ª\u21aa]', '', content)

    # 5. ğŸŸ¢ ç›®å½•é¡µç å¯¹é½
    # å°†è¢« OCR åˆ‡æ–­çš„é¡µç æ•°å­—æ‹‰å›ä¸Šä¸€è¡Œ
    content = re.sub(r'(\d+\.[\d\.]*.*)\n+(\d+)$', r'\1 \2', content, flags=re.MULTILINE)
    # å¤„ç†ç›®å½•ç‚¹å·ï¼šTitle .... 12 -> Title 12
    content = re.sub(r'\.{3,}\s*(\d+)', r' \1', content)
    
    # 6. ğŸŸ¢ ç»“æ„å‹ç¼©
    content = re.sub(r'\n{3,}', '\n\n', content)

    # 7. ğŸŸ¢ è¿˜åŸå›¾ç‰‡
    for i, raw in enumerate(imgs):
        content = content.replace(f"__IMG_TMP_{i}__", raw)
    
    return content.strip()

# ==================== ğŸŸ¢ æ ¸å¿ƒä¿®å¤ï¼šæ™ºèƒ½æ–‡æœ¬åˆ‡åˆ†å™¨ ====================
def smart_chunk_text(text, max_chars=2000):
    """
    ä¼˜å…ˆæŒ‰åŒæ¢è¡Œ(\n\n)åˆ‡åˆ†æ®µè½ã€‚
    å¦‚æœæ®µè½å¤ªé•¿ï¼Œå†æŒ‰å•æ¢è¡Œ(\n)åˆ‡åˆ†ã€‚
    å°½æœ€å¤§åŠªåŠ›ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚
    """
    # 1. å…ˆæŒ‰â€œåŒæ¢è¡Œâ€åˆ‡åˆ†æˆå¤§æ®µè½ (è¿™æ˜¯æœ€è‡ªç„¶çš„è¯­ä¹‰è¾¹ç•Œ)
    paragraphs = text.split('\n\n')
    
    batches = []
    current_batch = []
    current_length = 0

    # ğŸŸ¢ çœŸæ­£è°ƒç”¨å‡½æ•°ï¼šæ£€æµ‹å½“å‰å—æ˜¯å¦å±äºç›®å½•æ¨¡å¼
    is_toc_mode = is_likely_toc(text)
    
    for para in paragraphs:
        # å¦‚æœè¿™æ˜¯ä¸€ä¸ªå›¾ç‰‡è¡Œ (![...])ï¼Œå°½é‡è®©å®ƒå•ç‹¬æˆæ®µæˆ–è€…è·Ÿéšä¸Šä¸€æ®µ
        # ä½†ä¸è¦æŠŠå®ƒç¡¬ç”Ÿç”Ÿåˆ‡åˆ°ä¸‹ä¸€æ‰¹æ¬¡å¦‚æœè¿˜èƒ½æ”¾å¾—ä¸‹
        
        para_len = len(para)

        # ğŸŸ¢ ç›®å½•ä¼˜åŒ–ï¼šå¦‚æœæ˜¯ç›®å½•æ¨¡å¼ï¼Œä¸”é‡åˆ°ç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚ "1 Introduction"ï¼‰
        # åˆ™å¼ºåˆ¶å¼€å¯æ–°å—ï¼Œé¿å…æŠŠç›®å½•çš„ä¸åŒç« èŠ‚æ··åœ¨ä¸€èµ·ç¿»è¯‘å¯¼è‡´æ•£ä¹±
        if is_toc_mode and re.match(r'^\d+\s+[A-Z\u4e00-\u9fa5]', para.strip()):
            if current_batch:
                batches.append("\n\n".join(current_batch))
                current_batch = []
                current_length = 0
        
        # æƒ…å†µ A: å½“å‰æ®µè½æœ¬èº«å°±è¶…é•¿ (ä¾‹å¦‚ > 2000å­—ç¬¦çš„å¤§é•¿ç¯‡ OCR ç»“æœ)
        # éœ€è¦å†…éƒ¨å†åˆ‡åˆ† (æŒ‰å•æ¢è¡Œåˆ‡)
        if para_len > max_chars:
            # å…ˆæŠŠä¹‹å‰æ”’çš„å­˜èµ·æ¥
            if current_batch:
                batches.append("\n\n".join(current_batch))
                current_batch = []
                current_length = 0
            
            # å†…éƒ¨åˆ‡åˆ†é€»è¾‘
            lines = para.split('\n')
            temp_chunk = []
            temp_len = 0
            for line in lines:
                if temp_len + len(line) > max_chars and temp_chunk:
                    batches.append("\n".join(temp_chunk))
                    temp_chunk = [line]
                    temp_len = len(line)
                else:
                    temp_chunk.append(line)
                    temp_len += len(line)
            if temp_chunk:
                batches.append("\n".join(temp_chunk))
                
        # æƒ…å†µ B: å½“å‰æ®µè½ä¸è¶…é•¿ï¼Œä½†åŠ ä¸Šå»ä¼šè¶…è¿‡ Batch é™åˆ¶
        elif current_length + para_len > max_chars and current_batch:
            batches.append("\n\n".join(current_batch))
            current_batch = [para]
            current_length = para_len
            
        # æƒ…å†µ C: å®‰å…¨ï¼ŒåŠ å…¥å½“å‰ Batch
        else:
            current_batch.append(para)
            current_length += para_len
            
    # å¤„ç†å‰©ä½™éƒ¨åˆ†
    if current_batch:
        batches.append("\n\n".join(current_batch))
        
    return batches

# ==================== ğŸŸ¢ æ ¸å¿ƒä¿®å¤ï¼šç‹¬ç«‹ç¿»è¯‘è¾…åŠ©å‡½æ•° ====================
def translate_chunk(text_chunk):
    """
    è¿™æ˜¯ä¸€ä¸ªå…¨å±€å‡½æ•°ï¼Œç¡®ä¿ ThreadPoolExecutor å¯ä»¥ç¨³å®šè°ƒç”¨ã€‚
    """
    if not text_chunk.strip():
        return ""
    try:
        # å®ä¾‹åŒ–æ–°çš„ SafeTranslator (å®ƒç°åœ¨åŒ…å« Advanced Isolation é€»è¾‘)
        local_translator = SafeTranslator()
        return local_translator.translate_bilingual(text_chunk)
    except Exception as e:
        print(f"âŒ æ‰¹æ¬¡ç¿»è¯‘å¤±è´¥: {e}")
        # å¦‚æœç¿»è¯‘æŒ‚äº†ï¼Œè‡³å°‘è¿”å›åŸæ–‡ï¼Œä¸è¦è®©ç”¨æˆ·çœ‹åˆ°æŠ¥é”™å †æ ˆ
        return text_chunk

# ==============================================================================
# ğŸŸ¢ ç¬¬å››éƒ¨åˆ†ï¼šOCR å¼•æ“æ¨¡å— (Mistral OCR)
# ==============================================================================

def get_mistral_client():
    """è·å–é…ç½®å¥½çš„ Mistral å®¢æˆ·ç«¯"""
    if not MISTRAL_API_KEY or "æ‚¨çš„" in MISTRAL_API_KEY:
        raise ValueError("è¯·åœ¨ app.py ä¸­å¡«å†™æœ‰æ•ˆçš„ Mistral API Key")
    return Mistral(api_key=MISTRAL_API_KEY)

def process_chunk_with_mistral(file_content_bytes, mime_type, filename_base):
    """
    åŠŸèƒ½ï¼šè°ƒç”¨ Mistral OCR API å¤„ç†å•ä¸ª PDF/å›¾ç‰‡å—ã€‚
    
    :param file_content_bytes: æ–‡ä»¶äºŒè¿›åˆ¶æ•°æ®
    :param mime_type: æ–‡ä»¶ç±»å‹ (application/pdf æˆ– image/...)
    :param filename_base: æ–‡ä»¶åï¼ˆç”¨äºæ—¥å¿—ï¼‰
    :return: åŒ…å« Base64 å›¾ç‰‡çš„ Markdown å­—ç¬¦ä¸²
    """
    try:
        # 1. ç¼–ç ä¸º Base64 Data URI
        base64_encoded = base64.b64encode(file_content_bytes).decode('utf-8')
        data_uri = f"data:{mime_type};base64,{base64_encoded}"

        client = get_mistral_client()
        
        # 2. è°ƒç”¨ API
        ocr_response = client.ocr.process(
            model="mistral-ocr-latest",
            document={
                "type": "document_url",
                "document_url": data_uri
            },
            include_image_base64=True
        )
        
        full_markdown = ""
        image_map = {}
        
        # 3. è§£æç»“æœï¼Œæå– Markdown å’Œå›¾ç‰‡
        for page in ocr_response.pages:
            for img in page.images:
                image_map[img.id] = img.image_base64
            
            # ğŸŸ¢ æ·»åŠ è‡ªå®šä¹‰åˆ†é¡µæ ‡è®°ï¼Œç”¨äºå‰ç«¯åŒæ­¥æ»šåŠ¨
            full_markdown += f"\n\n[[PAGE_BREAK]]\n\n{page.markdown}"

        # 4. å°† Markdown ä¸­çš„å›¾ç‰‡ ID æ›¿æ¢ä¸º Base64
        def replace_img_ref(match):
            img_id = match.group(1)
            if img_id in image_map:
                b64_data = image_map[img_id]
                if not b64_data.startswith("data:"):
                    b64_data = f"data:image/jpeg;base64,{b64_data}"
                return f"![image]({b64_data})"
            return match.group(0)

        final_markdown = re.sub(r'!\[.*?\]\((.*?)\)', replace_img_ref, full_markdown)
        return final_markdown
        
    except Exception as e:
        print(f"âŒ Mistral å¤„ç† {filename_base} å¤±è´¥: {e}")
        return f"# âŒ è§£æå¤±è´¥: {str(e)}\n\n"

# ==============================================================================
# ğŸŸ¢ ç¬¬äº”éƒ¨åˆ†ï¼šFlask è·¯ç”±æ§åˆ¶å™¨ (Routes)
# ==============================================================================

@app.route('/')
def index():
    """æ¸²æŸ“ä¸»é¡µ"""
    return render_template('index.html')

@app.route('/gh_proxy')
def gh_proxy():
    """
    GitHub æ–‡ä»¶ä»£ç†æ¥å£
    åŠŸèƒ½ï¼šå‰ç«¯ç›´æ¥è¯·æ±‚ GitHub ä¼šæœ‰è·¨åŸŸå’Œé‰´æƒé—®é¢˜ï¼Œé€šè¿‡æ­¤æ¥å£ä¸­è½¬ã€‚
    å‚æ•°ï¼špath (GitHubæ–‡ä»¶è·¯å¾„), download (true/false)
    """
    path = request.args.get('path')
    should_download = request.args.get('download', 'false').lower() == 'true'
    
    if not path: return "No path specified", 400
    
    url = f"{GITHUB_API_BASE}/{urllib.parse.quote(path)}"
    
    try:
        # 1. è·å–æ–‡ä»¶å…ƒæ•°æ® (å« download_url)
        meta_resp = requests.get(url, headers=GH_HEADERS)
        if meta_resp.status_code != 200: 
            return f"File not found on GitHub: {meta_resp.text}", 404
        
        # 2. ä¸‹è½½å®é™…æ–‡ä»¶å†…å®¹
        download_url = meta_resp.json().get('download_url')
        file_resp = requests.get(download_url, headers=GH_HEADERS)
        
        # 3. æ„é€ å“åº”ç±»å‹
        mimetype = 'text/plain'
        if path.endswith('.pdf'): mimetype = 'application/pdf'
        elif path.endswith('.md'): mimetype = 'text/markdown'
        elif path.endswith(('.jpg', '.png')): mimetype = 'image/jpeg'
        
        response = Response(file_resp.content, mimetype=mimetype)

        # 4. å¦‚æœè¯·æ±‚ä¸‹è½½ï¼Œæ·»åŠ é™„ä»¶å¤´
        if should_download:
            filename = os.path.basename(path)
            encoded_filename = urllib.parse.quote(filename)
            response.headers["Content-Disposition"] = f"attachment; filename*=utf-8''{encoded_filename}"
        
        return response

    except Exception as e:
        traceback.print_exc()
        return f"Proxy Error: {e}", 500

@app.route('/history/preload', methods=['POST'])
def preload_history():
    """
    ğŸŸ¢ æ–°å¢æ¥å£ï¼šé¢„åŠ è½½å†å²è®°å½•
    å‰ç«¯é€‰æ‹©èº«ä»½åç«‹å³è°ƒç”¨æ­¤æ¥å£ï¼Œåç«¯å¼€å¯çº¿ç¨‹å» GitHub æ‹‰å–æ•°æ®ã€‚
    """
    user_id = request.json.get('user', 's1')
    
    # å¼€å¯çº¿ç¨‹è¿›è¡Œåå°æ›´æ–°ï¼Œç«‹å³è¿”å›ï¼Œä¸é˜»å¡å‰ç«¯
    thread = threading.Thread(target=background_refresh_task, args=(user_id,))
    thread.start()
    
    return jsonify({'status': 'started', 'message': f'Background fetch started for {user_id}'})
@app.route('/history/list', methods=['GET'])
def get_history_list():
    """
    ä¿®æ”¹åçš„åˆ—è¡¨æ¥å£ï¼šä¼˜å…ˆè¯»ç¼“å­˜
    """
    user_id = request.args.get('user', 's1')
    
    # ä¼˜å…ˆä»ç®¡ç†å™¨è¯»å–
    items = history_manager.get(user_id)
    
    if items:
        print(f"âš¡ [Cache Hit] å‘½ä¸­æŒä¹…åŒ–ç¼“å­˜: {user_id}")
    else:
        print(f"ğŸ¢ [Cache Miss] ç¼“å­˜å¤±æ•ˆï¼Œæ­£åœ¨åŒæ­¥...")
        items = _fetch_github_data(user_id)
        history_manager.set(user_id, items)
    
    # 3. è¡¥å…¨ URL (url_for éœ€è¦åœ¨è¯·æ±‚ä¸Šä¸‹æ–‡ä¸­è¿è¡Œ)
    # å› ä¸ºç¼“å­˜é‡Œå­˜çš„æ˜¯ pathï¼Œè¿™é‡ŒåŠ¨æ€ç”Ÿæˆæœ€ç»ˆ URL
    final_items = []
    for item in items:
        # æµ…æ‹·è´ä¸€ä¸‹ï¼Œé¿å…ä¿®æ”¹ç¼“å­˜é‡Œçš„åŸå§‹æ•°æ®
        new_item = item.copy() 
        new_item['pdf_url'] = url_for('gh_proxy', path=item['pdf_path'])
        new_item['md_url'] = url_for('gh_proxy', path=item['md_path'])
        final_items.append(new_item)

    return jsonify(final_items)

@app.route('/upload', methods=['POST'])
def upload_file():
    """
    æ ¸å¿ƒæ¥å£ï¼šä¸Šä¼ ä¸ OCR å¤„ç†
    æµç¨‹ï¼šä¸Šä¼  -> ä¸´æ—¶å­˜å‚¨ -> PDFæ‹†åˆ† -> å¾ªç¯OCR -> åˆå¹¶Markdown -> ä¸Šä¼ GitHub -> æ¸…ç†
    """
    # 1. éªŒè¯è¯·æ±‚
    user_id = request.form.get('user', 's1')
    if user_id not in USERS: return jsonify({'error': 'Invalid user'}), 403
    if 'file' not in request.files: return jsonify({'error': 'No file uploaded'}), 400
    
    file = request.files['file']
    filename = file.filename
    file_extension = filename.rsplit('.', 1)[-1].lower()
    
    # 2. å‡†å¤‡è·¯å¾„ä¸ ID
    timestamp = int(time.time())
    filename_base = os.path.splitext(filename)[0]
    task_id = f"{filename_base}_{timestamp}"
    
    # GitHub å­˜å‚¨è·¯å¾„
    gh_pdf_path = f"{user_id}/{task_id}.{file_extension}"
    gh_md_path = f"{user_id}/{task_id}.md"

    temp_filepath = None
    temp_md_path = None
    all_markdown_chunks = []

    try:
        # 3. ä¿å­˜ä¸Šä¼ æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        temp_dir = tempfile.gettempdir()
        temp_filepath = os.path.join(temp_dir, f"{task_id}.{file_extension}")
        file.save(temp_filepath)
        
        final_markdown = ""

        # 4. æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†
        if file_extension == 'pdf':
            # --- PDF å¤„ç†æµç¨‹ (åˆ†å—) ---
            reader = PdfReader(temp_filepath)
            total_pages = len(reader.pages)
            
            for start_page in range(0, total_pages, PAGE_CHUNK_SIZE):
                end_page = min(start_page + PAGE_CHUNK_SIZE, total_pages)
                page_range_str = f"P{start_page+1}-P{end_page}"
                
                # åˆ›å»ºä¸´æ—¶åˆ†å—æ–‡ä»¶
                writer = PdfWriter()
                for i in range(start_page, end_page):
                    writer.add_page(reader.pages[i])
                
                temp_chunk_path = os.path.join(temp_dir, f"{task_id}_{page_range_str}.pdf")
                with open(temp_chunk_path, "wb") as output_stream:
                    writer.write(output_stream)
                
                # è¯»å–åˆ†å—å¹¶è°ƒç”¨ OCR
                with open(temp_chunk_path, "rb") as chunk_file:
                    chunk_bytes = chunk_file.read()
                
                print(f"ğŸ”„ Processing chunk: {page_range_str}")
                markdown_chunk = process_chunk_with_mistral(
                    chunk_bytes, "application/pdf", f"{task_id}_{page_range_str}"
                )
                
                all_markdown_chunks.append(markdown_chunk)
                os.remove(temp_chunk_path) # æ¸…ç†åˆ†å—
            
            # åˆå¹¶ç»“æœï¼Œä½¿ç”¨åŒæ­¥æ ‡è®°
            final_markdown = "\n----------\n".join(all_markdown_chunks)
        
        elif file_extension in ['jpg', 'jpeg', 'png']:
            # --- å›¾ç‰‡å¤„ç†æµç¨‹ ---
            with open(temp_filepath, "rb") as image_file:
                chunk_bytes = image_file.read()
            final_markdown = process_chunk_with_mistral(
                chunk_bytes, f"image/{file_extension}", task_id
            )
        
        if not final_markdown: final_markdown = "# âš ï¸ è¯†åˆ«å†…å®¹ä¸ºç©º"

        if final_markdown:
            # ğŸŸ¢ å¿…é¡»åœ¨è¿™é‡Œè°ƒç”¨æ¸…æ´—å‡½æ•°ï¼Œä¿®å¤ä¸Šä¼ åçš„åŸå§‹ MD
            final_markdown = backend_smart_clean(final_markdown)

        # 5. ä¸Šä¼ ç»“æœåˆ° GitHub
        # 5.1 ä¸Šä¼ æºæ–‡ä»¶ (PDF/Image)
        print(f"â˜ï¸ Uploading source to {gh_pdf_path}...")
        if not upload_to_github(temp_filepath, gh_pdf_path, f"Add source: {filename}"):
             raise Exception("Failed to upload source file.")
        
        # 5.2 ä¸Šä¼  Markdown
        temp_md_path = os.path.join(temp_dir, f"{task_id}.md")
        with open(temp_md_path, "w", encoding="utf-8") as f:
            f.write(final_markdown)
        
        print(f"â˜ï¸ Uploading markdown to {gh_md_path}...")
        if not upload_to_github(temp_md_path, gh_md_path, f"Add markdown: {filename}"):
             raise Exception("Failed to upload markdown.")
        
        # ğŸŸ¢ æ ¸å¿ƒä¿®æ”¹ï¼šä¸Šä¼ æˆåŠŸåï¼Œè®©ç¼“å­˜å¤±æ•ˆæˆ–ç«‹å³åˆ·æ–°
        # æ–¹æ¡ˆï¼šå¼€å¯ä¸€ä¸ªçº¿ç¨‹ï¼Œç¨ååˆ·æ–°è¯¥ç”¨æˆ·çš„ç¼“å­˜
        print(f"â™»ï¸ ä¸Šä¼ æˆåŠŸï¼Œè§¦å‘åå°ç¼“å­˜åˆ·æ–°: {user_id}")
        refresh_thread = threading.Thread(target=background_refresh_task, args=(user_id,))
        refresh_thread.start()
        
        # 6. è¿”å›ç»“æœ
        return jsonify({
            'markdown': final_markdown,
            'download_url': url_for('gh_proxy', path=gh_md_path, download='true'),
            'pdf_url': url_for('gh_proxy', path=gh_pdf_path),
            'gh_path': gh_md_path
        })

    except Exception as e:
        traceback.print_exc()
        return jsonify({'error': f"Processing Error: {str(e)}"}), 500
        
    finally:
        # 7. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_filepath and os.path.exists(temp_filepath):
            os.remove(temp_filepath)
        if temp_md_path and os.path.exists(temp_md_path):
            os.remove(temp_md_path)

@app.route('/translate', methods=['POST'])
def translate_file():
    """
    ç¿»è¯‘æ¥å£
    é€»è¾‘ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰ç¿»è¯‘ç¼“å­˜ -> è‹¥æ— ï¼Œä¸‹è½½åŸMD -> åˆ†æ®µç¿»è¯‘ -> ä¸Šä¼ æ–°MD
    """
    data = request.get_json()
    if not data or 'path' not in data:
        return jsonify({"error": "Missing path parameter"}), 400
    
    gh_path = data.get('path') 
    dual_path = gh_path.replace('.md', '_dual.md')
    
    try:
        # 1. æ£€æŸ¥ GitHub æ˜¯å¦å·²æœ‰ç¿»è¯‘ç¼“å­˜
        check_url = f"{GITHUB_API_BASE}/{urllib.parse.quote(dual_path)}"
        if requests.get(check_url, headers=GH_HEADERS).status_code == 200:
            print("âœ… Cache hit for translation.")
            download_url = requests.get(check_url, headers=GH_HEADERS).json().get('download_url')
            return jsonify({
                'content': requests.get(download_url, headers=GH_HEADERS).text, 
                'status': 'cached',
                'dual_url': url_for('gh_proxy', path=dual_path, download='true')
            })

        # 2. ä¸‹è½½åŸå§‹ Markdown
        original_meta_url = f"{GITHUB_API_BASE}/{urllib.parse.quote(gh_path)}"
        meta_resp = requests.get(original_meta_url, headers=GH_HEADERS)
        if meta_resp.status_code != 200: return jsonify({'error': 'Original file not found'}), 404
        
        original_content = requests.get(meta_resp.json().get('download_url'), headers=GH_HEADERS).text
        
        # 1. åç«¯æ¸…æ´— (åŒæ­¥ä¹‹å‰å‰ç«¯çš„æ¸…æ´—é€»è¾‘)
        clean_content = backend_smart_clean(original_content)
        
        # 2. æ™ºèƒ½åˆ†å— (æŒ‰è¯­ä¹‰/é•¿åº¦åˆ‡åˆ†)
        batches = smart_chunk_text(clean_content, max_chars=2000)
        
        print(f"ğŸš€ å¼€å§‹å¹¶å‘ç¿»è¯‘ï¼Œå…± {len(batches)} ä¸ªæ‰¹æ¬¡...")

        # 3. ä½¿ç”¨å¹¶å‘æ‰§è¡Œå…¨å±€è¾…åŠ©å‡½æ•°
        with ThreadPoolExecutor(max_workers=8) as executor:
            # ä½¿ç”¨å…¨å±€å‡½æ•° translate_chunk é¿å…é—­åŒ…å¼•ç”¨é”™è¯¯
            dual_chunks = list(executor.map(translate_chunk, batches))

        dual_content = "\n\n".join(dual_chunks)

        # é‡æ–°ç»„åˆ
        dual_content = "\n\n".join(dual_chunks)
        
        # 4. ä¸Šä¼ ç¿»è¯‘ç»“æœ
        temp_dual_path = os.path.join(tempfile.gettempdir(), "temp_dual.md")
        with open(temp_dual_path, "w", encoding="utf-8") as f:
            f.write(dual_content)
            
        print(f"â˜ï¸ Uploading translation to {dual_path}...")
        upload_to_github(temp_dual_path, dual_path, "Add AI Translation")
        os.remove(temp_dual_path)
        
        return jsonify({
            'content': dual_content, 
            'status': 'translated',
            'dual_url': url_for('gh_proxy', path=dual_path, download='true')
        })

    except Exception as e:
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500
    
@app.route('/history/delete', methods=['POST'])
def delete_history():
    data = request.json
    user_id = data.get('user')
    pdf_path = data.get('pdf_path')
    md_path = data.get('md_path')
    
    if not user_id or not pdf_path or not md_path:
        return jsonify({'error': 'å‚æ•°ä¸å®Œæ•´'}), 400

    try:
        # 1. å®šä¹‰éœ€è¦å°è¯•åˆ é™¤çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        # åŒ…å« PDFã€åŸå§‹ MD å’Œå¯èƒ½å­˜åœ¨çš„ åŒè¯­ç‰ˆ MD
        files_to_delete = [
            pdf_path, 
            md_path, 
            md_path.replace('.md', '_dual.md')
        ]
        
        results = []
        for path in files_to_delete:
            # 2. è·å–æ–‡ä»¶çš„ SHA å€¼ï¼ˆGitHub åˆ é™¤æ–‡ä»¶å¿…é¡»æä¾› SHAï¼‰
            url = f"{GITHUB_API_BASE}/{urllib.parse.quote(path)}"
            resp = requests.get(url, headers=GH_HEADERS)
            
            if resp.status_code == 200:
                sha = resp.json().get('sha')
                
                # 3. æ‰§è¡Œåˆ é™¤æ“ä½œ
                del_payload = {
                    "message": f"ğŸ—‘ï¸ å½»åº•åˆ é™¤æ–‡æ¡£: {path}",
                    "sha": sha,
                    "branch": GITHUB_BRANCH
                }
                del_resp = requests.delete(url, json=del_payload, headers=GH_HEADERS)
                results.append(f"{path}: {del_resp.status_code}")
            else:
                results.append(f"{path}: è·³è¿‡ (æ–‡ä»¶ä¸å­˜åœ¨)")

        # 4. å…³é”®ï¼šåˆ é™¤åå¿…é¡»å¼ºåˆ¶åˆ·æ–°æœ¬åœ°ç¼“å­˜
        # è¿™æ ·ä¸‹æ¬¡å‰ç«¯è¯·æ±‚åˆ—è¡¨æ—¶ï¼Œçœ‹åˆ°çš„å°±æ˜¯æ›´æ–°åçš„æ•°æ®
        print(f"â™»ï¸ æ–‡ä»¶åˆ é™¤æˆåŠŸï¼Œæ­£åœ¨åˆ·æ–° {user_id} çš„ç¼“å­˜...")
        _fetch_github_data(user_id)
        
        return jsonify({
            'status': 'success', 
            'details': results,
            'message': 'æ–‡ä»¶å·²ä» GitHub ç‰©ç†åˆ é™¤å¹¶åŒæ­¥ç¼“å­˜'
        })
        
    except Exception as e:
        print(f"âŒ åˆ é™¤å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True, port=5000)
